{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b2aab-3df7-4b34-a7e3-57fd793a16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImportLocalData import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db67821-8a58-4bcf-b1da-16452b3c09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BalanceClassDistribution import AdjustClassSamples, NumberOfSamplesClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37209ccb-297a-4e3f-9318-4e497f4c9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import one of the custom STKG files\n",
    "# Call BalanceClass... to handle outliers if you need\n",
    "# Please change path names based on your local files \n",
    "data = loadData('.../node_features.txt', '.../edges.txt', '.../edge_features.txt', '.../node_labels.txt')\n",
    "data = AdjustClassSamples(data) #this is optional, yet in paper we used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14268fc0-a8fa-4dac-b723-bb601bd0f1dc",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c6e76-c6a2-4094-9c5a-7514dd9fec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM, BatchNorm1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921e639-ac76-41d3-9846-ba89dab767ea",
   "metadata": {},
   "source": [
    "TemporalSAGE model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac7d38-d7f0-4f0f-971f-cb6053e92753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TemporalSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden_size, out_channels):\n",
    "        super(TemporalSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        self.lstm = LSTM(hidden_channels, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(data.num_nodes, -1, x.size(1))\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)\n",
    "        return out        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bca81-6198-4a70-908e-fcacd0664bb0",
   "metadata": {},
   "source": [
    "Cosine similarity based data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fed98-8bf5-4424-88d1-6c9ebe94bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addEdgesSimilarity(data, similarity_threshold=0.8, max_new_edges=100):\n",
    "# Calculate cosine similarity for all node pairs\n",
    "    nodeFeatures = data.x.cpu().numpy()\n",
    "    similarity_matrix = cosine_similarity(nodeFeatures)\n",
    "    new_edges = []\n",
    "    numNodes = data.num_nodes\n",
    "    for i in range(numNodes):\n",
    "        for j in range(i+1, numNodes):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                new_edges.append((i, j))\n",
    "                if len(new_edges) >= max_new_edges:\n",
    "                    break\n",
    "        if len(new_edges) >= max_new_edges:\n",
    "            break\n",
    "    newEdgesTensor = torch.tensor(new_edges, dtype=torch.long).t().to(data.edge_index.device)\n",
    "    data.edge_index = torch.cat([data.edge_index, newEdgesTensor], dim=1)\n",
    "    return data\n",
    "\n",
    "def removeEdgesCentrality(data, remove_ratio=0.1):\n",
    "    G = nx.Graph()\n",
    "    edges = data.edge_index.t().cpu().numpy()\n",
    "    G.add_edges_from(edges)\n",
    "# Compute degree centrality for each node\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    edgeImportance = [(u, v, centrality[u] + centrality[v]) for u, v in G.edges]\n",
    "# Sort edges by importance and select edges to remove\n",
    "    edgeImportance.sort(key=lambda x: x[2])\n",
    "    removeCount = int(len(edgeImportance) * remove_ratio)\n",
    "    edgesRemove = edgeImportance[:removeCount]    \n",
    "# Remove low-centrality edges\n",
    "    for u, v, _ in edgesRemove:\n",
    "        G.remove_edge(u, v)    \n",
    "# Update edge_index after removal\n",
    "    data.edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().to(data.edge_index.device)    \n",
    "    return data\n",
    "\n",
    "\n",
    "# For balance class distribution    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma \n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CeLoss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-CeLoss)\n",
    "        # Focal Loss calculation\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * CeLoss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed4766-a5b9-497a-a740-18ea1ac8142e",
   "metadata": {},
   "source": [
    "Function for Top-1/ Top-5 accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69176345-292f-4ebd-8f0c-3628cb497534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "# Top-1 accuracy\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    top1_correct = preds.eq(labels).sum().item()\n",
    "    top1_acc = top1_correct / len(labels)    \n",
    "# Top-5 accuracy\n",
    "    _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "    top5_correct = top5_preds.eq(labels.view(-1, 1)).sum().item()\n",
    "    top5_acc = top5_correct / len(labels)\n",
    "    return top1_acc, top5_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50571997-cc39-4464-9004-463956417cb2",
   "metadata": {},
   "source": [
    "Data preperation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fb40a-eb35-4d1c-91fc-5dccc9a9d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(data.x.numpy())\n",
    "data.x = torch.tensor(x_scaled, dtype=torch.float)\n",
    "data = addEdgesSimilarity(data, similarity_threshold=0.8, max_new_edges=100)\n",
    "data = removeEdgesCentrality(data, remove_ratio=0.1)\n",
    "\n",
    "# Split data into train (80%), validation (10%), and test (10%)\n",
    "train_idx, test_idx = train_test_split(torch.arange(data.num_nodes), test_size=0.2, stratify=data.y.cpu().numpy())\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.125, stratify=data.y[train_idx].cpu().numpy())  # 0.125 * 0.8 = 0.1\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "val_idx = torch.tensor(val_idx, dtype=torch.long)\n",
    "test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "# Create subgraphs for each split\n",
    "trainSubGraph = subgraph(train_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "valSubGraph = subgraph(val_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "testSubGraph = subgraph(test_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "train_data = Data(x=data.x[train_idx], edge_index=trainSubGraph[0], y=data.y[train_idx])\n",
    "val_data = Data(x=data.x[val_idx], edge_index=valSubGraph[0], y=data.y[val_idx])\n",
    "test_data = Data(x=data.x[test_idx], edge_index=testSubGraph[0], y=data.y[test_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2355f-f2b4-47b7-9f20-41ce0b3f183a",
   "metadata": {},
   "source": [
    "Initialization of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272ecd7-e685-48da-bd89-2f73c5b10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader([train_data], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "test_loader = DataLoader([test_data], batch_size=batch_size)\n",
    "numNodeFeatures = data.num_node_features\n",
    "num_classes = len(data.y.unique())\n",
    "hidden_channels = 256\n",
    "lstm_hidden_size = 128\n",
    "\n",
    "model = TemporalSAGE(numNodeFeatures, hidden_channels, lstm_hidden_size, num_classes).to(device)\n",
    "optim= torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1000, gamma=0.5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_focal = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb250f7-72c1-4a81-accf-ec9a2358da34",
   "metadata": {},
   "source": [
    "Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cea5a4-08be-46d6-90f2-983856027443",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    epoch_loss = 0    \n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss_ce = loss_fn(out, batch.y.to(device))\n",
    "        loss_focal_value = loss_focal(out, batch.y.to(device))\n",
    "        loss = 0.1 * loss_ce + 0.9 * loss_focal_value\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_top1_correct = 0\n",
    "        val_top5_correct = 0\n",
    "        val_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out_val = model(batch)\n",
    "                val_loss1 = loss_fn(out_val, batch.y.to(device))\n",
    "                val_loss2 = loss_focal(out_val, batch.y.to(device))\n",
    "                val_loss = 0.1 * val_loss1 + 0.9 * val_loss2\n",
    "                val_losses.append(val_loss.item())\n",
    "                top1_acc, top5_acc = calculate_accuracy(out_val, batch.y.to(device))\n",
    "                val_top1_correct += top1_acc * len(batch.y)\n",
    "                val_top5_correct += top5_acc * len(batch.y)\n",
    "                val_sum += len(batch.y)\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_top1Accuracy = val_top1_correct / val_sum\n",
    "            val_top5Accuracy = val_top5_correct / val_sum\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val Top-1 Acc: {val_top1Accuracy:.4f}, Val Top-5 Acc: {val_top5Accuracy:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e352942-31e9-460f-bb21-56cf01a49286",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e3c0e-9f7b-45a2-9733-67d215ba46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_top1_correct = 0\n",
    "test_top5_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out_test = model(batch)\n",
    "        top1_acc, top5_acc = calculate_accuracy(out_test, batch.y.to(device))\n",
    "        test_top1_correct += top1_acc * len(batch.y)\n",
    "        test_top5_correct += top5_acc * len(batch.y)\n",
    "        test_total += len(batch.y)\n",
    "test_top1Accuracy = test_top1_correct / test_total\n",
    "test_top5Accuracy = test_top5_correct / test_total\n",
    "\n",
    "print(f'\\nTest Top-1 Accuracy: {test_top1Accuracy:.4f}')\n",
    "print(f'Test Top-5 Accuracy: {test_top5Accuracy:.4f}')\n",
    "torch.save(model.state_dict(), 'TemporalSAGE_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a69347-7393-4c5a-8f96-27598594fe23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
