{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859550fd-0a1c-4a0f-8cdf-da4e5f6bedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImportLocalData import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da271cee-5382-436d-b040-d1a74965f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BalanceClassDistribution import AdjustClassSamples, NumberOfSamplesClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3b6f0-f6f4-4a1d-bd4d-18916daa3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import one of the custom KG files\n",
    "# Call BalanceClass... to handle outliers if you need\n",
    "# Please change path names based on your local files \n",
    "data = loadData('.../node_features.txt', '.../edges.txt', '.../edge_features.txt', '.../node_labels.txt')\n",
    "#data = AdjustClassSamples(data) #this is optional, yet in paper we used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f10ce-0951-41ac-9553-998de8e6976e",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a37f4e-2d96-450d-8f62-617614f0a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM, BatchNorm1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f54a0c-e17e-4d6c-b795-28f2c0ff99d9",
   "metadata": {},
   "source": [
    "Definition of TemporalSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd922545-061b-459c-812e-b0c9155da7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TemporalSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden_size, out_channels):\n",
    "        super(TemporalSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        self.lstm = LSTM(hidden_channels, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(data.num_nodes, -1, x.size(1))\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913952e9-b8d0-462d-88c1-caf5a49849cd",
   "metadata": {},
   "source": [
    "Ruled-based data augmentation: we propose adding edges by cosine similarity and removing edges by degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e506d6da-ce5c-45d4-94af-169e92ea239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEdgesSimilarity(data, similarity_threshold=0.8, max_new_edges=100):\n",
    "    # Calculate cosine similarity for all node pairs\n",
    "    nodeFeatures = data.x.cpu().numpy()\n",
    "    similarity_matrix = cosine_similarity(nodeFeatures)\n",
    "    new_edges = []\n",
    "    numNodes = data.num_nodes\n",
    "    for i in range(numNodes):\n",
    "        for j in range(i+1, numNodes):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                new_edges.append((i, j))\n",
    "                if len(new_edges) >= max_new_edges:\n",
    "                    break\n",
    "        if len(new_edges) >= max_new_edges:\n",
    "            break\n",
    "    newEdgesTensor = torch.tensor(new_edges, dtype=torch.long).t().to(data.edge_index.device)\n",
    "    data.edge_index = torch.cat([data.edge_index, newEdgesTensor], dim=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f3d6c7-f3ee-404f-b3ed-3935e6dd52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEdgesCentrality(data, remove_ratio=0.1):\n",
    "    G = nx.Graph()\n",
    "    edges = data.edge_index.t().cpu().numpy()\n",
    "    G.add_edges_from(edges)\n",
    "# Compute degree centrality for each node\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    edgeImportance = [(u, v, centrality[u] + centrality[v]) for u, v in G.edges]\n",
    "# Sort edges by importance and select edges to remove\n",
    "    edgeImportance.sort(key=lambda x: x[2])\n",
    "    removeCount = int(len(edgeImportance) * remove_ratio)\n",
    "    edgesRemove = edgeImportance[:removeCount]    \n",
    "# Remove low-centrality edges\n",
    "    for u, v, _ in edgesRemove:\n",
    "        G.remove_edge(u, v)    \n",
    "# Update edge_index after removal\n",
    "    data.edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().to(data.edge_index.device)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976fbd0-8ddd-4a93-ba4f-b3d7af6305d3",
   "metadata": {},
   "source": [
    "Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c60f837-0744-4bac-908e-dc9537c07d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(data.x.numpy())\n",
    "data.x = torch.tensor(x_scaled, dtype=torch.float)\n",
    "#agmentation based on similarity (specialized add and remove data as told in paper)\n",
    "data = addEdgesSimilarity(data, similarity_threshold=0.8, max_new_edges=100)\n",
    "data = removeEdgesCentrality(data, remove_ratio=0.1)\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "# For repporting inductive and tranductive results since it is stream based approach\n",
    "inductiveResults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "transductiveResults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "# List to store all results for writing to file\n",
    "log_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5030a6d-9b81-4a9a-b98c-bab43b5ac62a",
   "metadata": {},
   "source": [
    "Focal Loss definition for composite loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6882f8ff-6443-48a5-9e22-fac562533d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To balance class ditribution    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma \n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CeLoss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-CeLoss)\n",
    "        # Focal Loss calculation\n",
    "        FLoss = self.alpha * (1 - pt) ** self.gamma * CeLoss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(FLoss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(FLoss)\n",
    "        else:\n",
    "            return FLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2660cd6-8add-43ae-b150-0a2dc1f115ad",
   "metadata": {},
   "source": [
    "Definition of Evaluation Metrics with Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2063f4f3-70fa-421b-a1e5-fe145221563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR definition\n",
    "def meanReciprocalRank(y_true, y_prob):\n",
    "    ranks = []\n",
    "    for true_label, prob in zip(y_true, y_prob):\n",
    "        rank = np.where(np.argsort(prob)[::-1] == true_label)[0][0] + 1\n",
    "        ranks.append(1 / rank)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "# Test model function for both inductive and transductive reasoning\n",
    "def test_model(model, test_loader, loss_fn, data_type=\"Inductive\"):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            out_test = model(batch)\n",
    "            test_loss1 = loss_fn(out_test, batch.y.to(device))\n",
    "            test_loss2 = loss_focal(out_test, batch.y.to(device))\n",
    "            test_loss = 0.1 * test_loss1 + 0.9 * test_loss2  # Composite Loss Function definition\n",
    "            test_losses.append(test_loss.item())\n",
    "            _, pred_test = out_test.max(dim=1)\n",
    "            test_correct += float(pred_test.eq(batch.y.to(device)).sum().item())\n",
    "            all_preds.extend(pred_test.cpu().numpy())\n",
    "            all_true.extend(batch.y.cpu().numpy())\n",
    "            all_probs.extend(F.softmax(out_test, dim=1).cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_accuracy = test_correct / len(all_true)\n",
    "    precision = precision_score(all_true, all_preds, average='weighted')\n",
    "    recall = recall_score(all_true, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_true, all_preds, average='weighted')\n",
    "    mrr = meanReciprocalRank(all_true, all_probs)\n",
    "\n",
    "    outputResults = [f'{data_type} Reasoning (Test Loss): {test_loss:.4f}',f'{data_type} Reasoning (Test Accuracy): {test_accuracy:.4f}',f'{data_type} Reasoning (Precision): {precision:.4f}', f'{data_type} Reasoning (Recall): {recall:.4f}',\n",
    "        f'{data_type} Reasoning(F1 Score): {f1:.4f}',\n",
    "        f'{data_type} Reasoning(MRR): {mrr:.4f}']\n",
    "    for result in outputResults:\n",
    "        print(result)\n",
    "\n",
    "    return test_accuracy, precision, recall, f1, mrr, outputResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc2f02-fb5d-4255-be11-b908e168be33",
   "metadata": {},
   "source": [
    "Training with Cross-validation to ensure robustness of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e010a-3941-4d8d-839c-06ed481e97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_val_idx, test_idx) in enumerate(skf.split(torch.arange(data.num_nodes), data.y.cpu().numpy())):\n",
    "    log_output.append(f'Fold {fold+1}/{k_folds}')\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    # train test split\n",
    "    train_idx, val_idx = train_test_split(train_val_idx, test_size=0.1, stratify=data.y[train_val_idx].cpu().numpy())\n",
    "    train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "    val_idx = torch.tensor(val_idx, dtype=torch.long)\n",
    "    test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "    # Inductive Reasoning (All information of test data will not be used during training)\n",
    "    trainSubgraph = subgraph(train_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    valSubgraph = subgraph(val_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    testSubgraph_inductive = subgraph(test_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    # Transductive Reasoning (All node and edge information is accessible during training, except for the labels of the test nodes)\n",
    "    globalSubgraph = subgraph(torch.arange(data.num_nodes), data.edge_index, relabel_nodes=False, num_nodes=data.num_nodes)\n",
    "    train_data = Data(x=data.x[train_idx], edge_index=trainSubgraph[0], y=data.y[train_idx])\n",
    "    val_data = Data(x=data.x[val_idx], edge_index=valSubgraph[0], y=data.y[val_idx])\n",
    "    Inductive_test_data = Data(x=data.x[test_idx], edge_index=testSubgraph_inductive[0], y=data.y[test_idx])\n",
    "    Transductive_test_data = Data(x=data.x, edge_index=globalSubgraph[0], y=data.y)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader([train_data], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "    Inductive_test_loader = DataLoader([Inductive_test_data], batch_size=batch_size)\n",
    "    Transductive_test_loader = DataLoader([Transductive_test_data], batch_size=batch_size)\n",
    "    num_nodeFeatures = data.num_node_features\n",
    "    numOfclasses = len(data.y.unique())\n",
    "    hidden_channels = 256\n",
    "    lstm_hid = 128\n",
    "    model = TemporalSAGE(num_nodeFeatures, hidden_channels, lstm_hid, numOfclasses).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=500, gamma=0.5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss_focal = FocalLoss()\n",
    "\n",
    "    for epoch in range(4000):\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss_ce = loss_fn(out, batch.y.to(device))\n",
    "            lossFocalValue = loss_focal(out, batch.y.to(device))\n",
    "            loss = 0.1 * loss_ce + 0.9 * lossFocalValue\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                val_correct = 0\n",
    "                for batch in val_loader:\n",
    "                    out_val = model(batch)\n",
    "                    val_loss1 = loss_fn(out_val, batch.y.to(device))\n",
    "                    val_loss2 = loss_focal(out_val, batch.y.to(device))\n",
    "                    val_loss = 0.1 * val_loss1 + 0.9 * val_loss2  # 0.1* cross entropy + 0.9*focal loss\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    _, pred_val = out_val.max(dim=1)\n",
    "                    val_correct += float(pred_val.eq(batch.y.to(device)).sum().item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "                val_accuracy = val_correct / len(val_data.y)\n",
    "            log_output.append(f'Epoch: {epoch+1}, Loss: {loss_ce.item()}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')  # Log for validation results\n",
    "            print(f'Epoch: {epoch+1}, Loss: {loss_ce.item()}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "            model.train()\n",
    "    inductive_accuracy, inductive_precision, inductive_recall, inductive_f1, inductive_mrr, inductive_log = test_model(model, Inductive_test_loader, loss_fn, \"Inductive\")\n",
    "    transductive_accuracy, transductive_precision, transductive_recall, transductive_f1, transductive_mrr, transductive_log = test_model(model, Trasnductive_test_loader, loss_fn, \"Transductive\")\n",
    "\n",
    "    inductiveResults['accuracy'].append(inductive_accuracy)\n",
    "    inductiveResults['precision'].append(inductive_precision)\n",
    "    inductiveResults['recall'].append(inductive_recall)\n",
    "    inductiveResults['f1'].append(inductive_f1)\n",
    "    inductiveResults['mrr'].append(inductive_mrr)\n",
    "\n",
    "    transductive_results['accuracy'].append(transductive_accuracy)\n",
    "    transductive_results['precision'].append(transductive_precision)\n",
    "    transductive_results['recall'].append(transductive_recall)\n",
    "    transductive_results['f1'].append(transductive_f1)\n",
    "    transductive_results['mrr'].append(transductive_mrr)\n",
    "\n",
    "    foldResults = f\"Fold {fold+1} Results - Inductive Accuracy: {inductive_accuracy:.4f}, Transductive Accuracy: {transductive_accuracy:.4f}\"\n",
    "    print(foldResults)\n",
    "    log_output.append(foldResults)  #Log for each fod results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b2669-5b06-456a-9701-691f5ff9955a",
   "metadata": {},
   "source": [
    "Calulating means and standard deviations of performance results of folds to report them in addition to main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765fff1-946f-48db-abbe-76602884dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_and_std(results_dict):\n",
    "    averages = {}\n",
    "    std_devs = {}\n",
    "    for key, values in results_dict.items():\n",
    "        averages[key] = np.mean(values)\n",
    "        std_devs[key] = np.std(values)\n",
    "    return averages, std_devs\n",
    "\n",
    "inductive_avg, inductive_std = calculate_average_and_std(inductiveResults)\n",
    "transductive_avg, transductive_std = calculate_average_and_std(transductiveResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f1852c1-8621-4c02-a5de-bb0505e8eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "output.append(f'Final Inductive Average Results - Accuracy: {inductive_avg[\"accuracy\"]:.4f}, Precision: {inductive_avg[\"precision\"]:.4f}, Recall: {inductive_avg[\"recall\"]:.4f}, F1: {inductive_avg[\"f1\"]:.4f}, MRR: {inductive_avg[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Inductive Standard Deviations - Accuracy: {inductive_std[\"accuracy\"]:.4f}, Precision: {inductive_std[\"precision\"]:.4f}, Recall: {inductive_std[\"recall\"]:.4f}, F1: {inductive_std[\"f1\"]:.4f}, MRR: {inductive_std[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Transductive Average Results - Accuracy: {transductive_avg[\"accuracy\"]:.4f}, Precision: {transductive_avg[\"precision\"]:.4f}, Recall: {transductive_avg[\"recall\"]:.4f}, F1: {transductive_avg[\"f1\"]:.4f}, MRR: {transductive_avg[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Transductive Standard Deviations - Accuracy: {transductive_std[\"accuracy\"]:.4f}, Precision: {transductive_std[\"precision\"]:.4f}, Recall: {transductive_std[\"recall\"]:.4f}, F1: {transductive_std[\"f1\"]:.4f}, MRR: {transductive_std[\"mrr\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b30cc-43f7-4d8e-9a48-a06e582d5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the output to a .txt file\n",
    "with open('.../results.txt', 'w') as f:\n",
    "    for line in log_output:\n",
    "        f.write(line + '\\n')\n",
    "    for line in output:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "# Print the results\n",
    "for line in output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d53dbe-2871-4806-a1a5-9be95a28281a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
