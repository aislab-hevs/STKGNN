{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15451d8-ed69-487a-bab9-d2828e85751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImportLocalData import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80a2853-1b9c-4e5c-8119-68e7c691897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BalanceClassDistribution import AdjustClassSamples, NumberOfSamplesClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55460737-e7de-4237-9d01-e72c76a81d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import one of the custom KG files\n",
    "# Call BalanceClass... to handle outliers if you need\n",
    "# Please change path names based on your local files \n",
    "data = loadData('.../node_features.txt', '/home/gozde/GEREKLI_DOSYALAR/HMDB-STKG/edges.txt', '.../edge_features.txt', '.../node_labels.txt')\n",
    "#data = AdjustClassSamples(data) #this is optional, yet in paper we used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f6617-914d-4d2d-98a2-088718e7dbdc",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b8ccf-87f1-4c4d-a8b4-1a7b9587a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GAE\n",
    "from torch.nn import LSTM\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d3b3c-e826-4055-bf0e-6e1fb55c405b",
   "metadata": {},
   "source": [
    "Definition of FusionGAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0357b3a8-7cdc-4a4f-bcec-ef36d60e3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#GAT-LSTM based activityy recognition module with GAE based reconstruction for deper anlysis\n",
    "class FusionGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden_size, out_channels):\n",
    "        super(FusionGAT, self).__init__()\n",
    "        # GAT-LSTM for node classification\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=2, concat=True)\n",
    "        self.conv2 = GATConv(hidden_channels * 2, hidden_channels, heads=1, concat=False)\n",
    "        self.lstm = LSTM(hidden_channels, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        # Graph Autoencoder for reconstruction loss\n",
    "        self.encoder_conv1 = GATConv(in_channels, hidden_channels, heads=2, concat=True)\n",
    "        self.encoder_conv2 = GATConv(hidden_channels * 2, hidden_channels, heads=1, concat=False)\n",
    "        self.fc_decoder = torch.nn.Linear(hidden_channels, in_channels)  # For node reconstruction\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "        x_cls = self.conv1(x, edge_index)\n",
    "        x_cls = F.elu(x_cls)\n",
    "        x_cls = self.dropout(x_cls)\n",
    "        x_cls = self.conv2(x_cls, edge_index)\n",
    "        x_cls = F.elu(x_cls)\n",
    "        x_cls = x_cls.view(data.num_nodes, -1, x_cls.size(1))\n",
    "        lstm_out, _ = self.lstm(x_cls)\n",
    "        out_cls = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        # Graph Autoencoder part\n",
    "        x_enc = self.encoder_conv1(x, edge_index)\n",
    "        x_enc = F.elu(x_enc)\n",
    "        x_enc = self.encoder_conv2(x_enc, edge_index)\n",
    "        out_reconstructed = self.fc_decoder(x_enc)\n",
    "\n",
    "        return out_cls, out_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfda6f4-83da-4464-8a90-92c73144503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(data.x.cpu().numpy())  # Scaling data on the CPU\n",
    "data.x = torch.tensor(x_scaled, dtype=torch.float).to(device)  # Move data back to GPU\n",
    "\n",
    "# Split data with balanced number of classes before 5-Fold Cross Validation \n",
    "# This is optional. you can change number of fold\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# For keeping inductive and transductive results\n",
    "inductive_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "transductive_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "\n",
    "# List to store all fold information and each epoch results for writing to file\n",
    "log_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34f100-87d1-4138-be8b-549b4e28a54d",
   "metadata": {},
   "source": [
    "Definition of Evaluation Metrics with Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e82a80-68ea-4b08-afb1-0a4bc92e15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR Definition\n",
    "def meanReciprocalRank(y_true, y_prob):\n",
    "    ranks = []\n",
    "    for true_label, prob in zip(y_true, y_prob):\n",
    "        rank = np.where(np.argsort(prob)[::-1] == true_label)[0][0] + 1\n",
    "        ranks.append(1 / rank)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "# Test model function for both inductive and transductive reasoning\n",
    "def test_model(model, test_loadr, loss_fn, reconsruction_loss_fn, data_type=\"Inductive\"):\n",
    "    model.eval()\n",
    "    testLosses = []\n",
    "    testCorrect = 0\n",
    "    allPreds = []\n",
    "    allTrue = []\n",
    "    allProbs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out_test_cls, out_test_recon = model(batch)\n",
    "            test_loss_cls = loss_fn(out_test_cls, batch.y)\n",
    "            test_loss_recon = reconstruction_loss_fn(out_test_recon, batch.x)\n",
    "            test_loss = test_loss_cls + 0.2 * test_loss_recon  # Composite Loss Function definition\n",
    "            testLosses.append(test_loss.item())\n",
    "            _, pred_test = out_test_cls.max(dim=1)\n",
    "            testCorrect += float(pred_test.eq(batch.y).sum().item())\n",
    "            \n",
    "            allPreds.extend(pred_test.cpu().numpy())\n",
    "            allTrue.extend(batch.y.cpu().numpy())\n",
    "            allProbs.extend(F.softmax(out_test_cls, dim=1).cpu().numpy())\n",
    "\n",
    "    testLoss = np.mean(testLosses)\n",
    "    testAccuracy = testCorrect / len(allTrue)\n",
    "\n",
    "    precision = precision_score(allTrue, allPreds, average='weighted')\n",
    "    recall = recall_score(allTrue, allPreds, average='weighted')\n",
    "    f1 = f1_score(allTrue, allPreds, average='weighted')\n",
    "    mrr = meanReciprocalRank(allTrue, allProbs)\n",
    "\n",
    "    outputResults = [f'{data_type} Reasoning (Test Los): {testLoss:.4f}', f'{data_type} Reasoning(Test Accuracy): {testAccuracy:.4f}', f'{data_type} Reasoning (Precision): {precision:.4f}',\n",
    "        f'{data_type} Reasoning (Recall): {recall:.4f}', f'{data_type} Reasoning (F1 Score): {f1:.4f}', f'{data_type} Reasoning(MRR): {mrr:.4f}']\n",
    "    \n",
    "    for result in outputResults:\n",
    "        print(result)\n",
    "    return test_accuracy, precision, recall, f1, mrr, output_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de226e-d013-4457-b268-c04d2baac2fe",
   "metadata": {},
   "source": [
    "Training with Cross-validation to ensure robustness of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8c205-e647-4453-9ff2-a8b2c3d375aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_val_idx, test_idx) in enumerate(skf.split(torch.arange(data.num_nodes), data.y.cpu().numpy())):\n",
    "    log_output.append(f'Fold {fold+1}/{k_folds}')  # Log for current fold\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # train test split\n",
    "    train_idx, val_idx = train_test_split(train_val_idx, test_size=0.1, stratify=data.y[train_val_idx].cpu().numpy())\n",
    "    train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "    val_idx = torch.tensor(val_idx, dtype=torch.long)\n",
    "    test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "    \n",
    "    # Inductive Reasoning: All information of test data will not be used during training\n",
    "    train_subgraph = subgraph(train_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    val_subgraph = subgraph(val_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    test_subgraph_inductive = subgraph(test_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    # Transductive Reasoning: All node and edge information is accessible during training, except for the labels of the test nodes\n",
    "    global_subgraph = subgraph(torch.arange(data.num_nodes), data.edge_index, relabel_nodes=False, num_nodes=data.num_nodes)\n",
    "    train_data = Data(x=data.x[train_idx], edge_index=train_subgraph[0], y=data.y[train_idx])\n",
    "    val_data = Data(x=data.x[val_idx], edge_index=val_subgraph[0], y=data.y[val_idx])\n",
    "    test_data_inductive = Data(x=data.x[test_idx], edge_index=test_subgraph_inductive[0], y=data.y[test_idx])\n",
    "    test_data_transductive = Data(x=data.x, edge_index=global_subgraph[0], y=data.y)\n",
    "\n",
    "    # Data Loader for PyTorch Geometric\n",
    "    batch_size = 32  #you can change batch_size\n",
    "    train_loader = DataLoader([train_data], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "    test_loader_inductive = DataLoader([test_data_inductive], batch_size=batch_size)\n",
    "    test_loader_transductive = DataLoader([test_data_transductive], batch_size=batch_size)\n",
    "\n",
    "    # Initializes the model:\n",
    "    # Uses the Adam algorithm for parameter optimization\n",
    "    # Performs dynamic learning rate adjustment\n",
    "    # Defines a suitable function for loss calculation    \n",
    "    num_node_features = data.num_node_features\n",
    "    num_classes = len(data.y.unique())\n",
    "    hidden_channels = 256\n",
    "    lstm_hidden_size = 128\n",
    "    model = FusionGAT(num_node_features, hidden_channels, lstm_hidden_size, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "    classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    reconstruction_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Training Phase\n",
    "    for epoch in range(4000): #you can change number of epoch\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out_cls, out_reconstructed = model(batch)\n",
    "\n",
    "            # Classification and Reconstruction Loss calculation\n",
    "            loss_cls = classification_loss_fn(out_cls, batch.y)\n",
    "            loss_recon = reconstruction_loss_fn(out_reconstructed, batch.x)\n",
    "\n",
    "            # Total composite loss calculation\n",
    "            loss = loss_cls + 0.2 * loss_recon\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Update Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Printing validation loss and accuracy every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                val_correct = 0\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    out_val_cls, out_val_recon = model(batch)\n",
    "                    val_loss_cls = classification_loss_fn(out_val_cls, batch.y)\n",
    "                    val_loss_recon = reconstruction_loss_fn(out_val_recon, batch.x)\n",
    "                    val_losses.append((val_loss_cls + 0.2 * val_loss_recon).item())\n",
    "                    _, pred_val = out_val_cls.max(dim=1)\n",
    "                    val_correct += float(pred_val.eq(batch.y).sum().item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "                val_accuracy = val_correct / len(val_data.y)\n",
    "            log_output.append(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "            print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "# Test stpe for each fold\n",
    "    inductiveAccuracy, inductivePrecision, inductiveRecall, inductive_f1, inductiveMrr, inductiveLog = test_model(model, test_loader_inductive, classification_loss_fn, reconstruction_loss_fn, \"Inductive\")\n",
    "    transductiveAccuracy, transductivePrecision, transductiveRecall, transductive_f1, transductiveMrr, transductive_log = test_model(model, test_loader_transductive, classification_loss_fn, reconstruction_loss_fn, \"Transductive\")\n",
    "    inductiveResults['accuracy'].append(inductiveAccuracy)\n",
    "    inductiveResults['precision'].append(inductivePrecision)\n",
    "    inductiveResults['recall'].append(inductiveRecall)\n",
    "    inductiveResults['f1'].append(inductive_f1)\n",
    "    inductiveResults['mrr'].append(inductiveMrr)\n",
    "    transductiveResults['accuracy'].append(transductiveRccuracy)\n",
    "    transductiveResults['precision'].append(transductiveRrecision)\n",
    "    transductiveResults['recall'].append(transductiveRecall)\n",
    "    transductiveResults['f1'].append(transductive_f1)\n",
    "    transductiveResults['mrr'].append(transductiveMrr)\n",
    "\n",
    "    foldResults = f\"Fold {fold+1} Results - Inductive Accuracy: {inductiveAccuracy:.4f}, Transductive Accuracy: {transductiveAccuracy:.4f}\"\n",
    "    print(foldResults)\n",
    "    log_output.append(foldResults)  # keep fold results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c9ebc-debf-4573-a989-eafdd1a32c32",
   "metadata": {},
   "source": [
    "Calculating means and standard deviations of performance results of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5681be8-5e54-4dc5-957d-437438abcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAverageStd(results_dict):\n",
    "    averages = {}\n",
    "    std_devs = {}\n",
    "    for key, values in results_dict.items():\n",
    "        averages[key] = np.mean(values)\n",
    "        std_devs[key] = np.std(values)\n",
    "    return averages, std_devs\n",
    "\n",
    "inductive_avg, inductive_std = calculateAverageStd(inductiveResults)\n",
    "transductive_avg, transductive_std = calculateAverageStd(transductiveResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228f30e-76fb-4785-822b-e1c413a2670d",
   "metadata": {},
   "source": [
    "Print the results to the terminal for observation the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ff0f7-68ff-438a-87cc-dda12dcae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "output.append(f'Final Inductive Average Results (Accuracy): {inductive_avg[\"accuracy\"]:.4f}, Precision: {inductive_avg[\"precision\"]:.4f}, Recall: {inductive_avg[\"recall\"]:.4f}, F1: {inductive_avg[\"f1\"]:.4f}, MRR: {inductive_avg[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Inductive Stadard Deviations (Accuracy): {inductive_std[\"accuracy\"]:.4f}, Precision: {inductive_std[\"precision\"]:.4f}, Recall: {inductive_std[\"recall\"]:.4f}, F1: {inductive_std[\"f1\"]:.4f}, MRR: {inductive_std[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Transductive Average Results (Accuracy): {transductive_avg[\"accuracy\"]:.4f}, Precision: {transductive_avg[\"precision\"]:.4f}, Recall: {transductive_avg[\"recall\"]:.4f}, F1: {transductive_avg[\"f1\"]:.4f}, MR: {transductive_avg[\"mrr\"]:.4f}')\n",
    "output.append(f'Final Tranductive Standard Devitions (Accuracy): {transductive_std[\"accuracy\"]:.4f}, Precision: {transductive_std[\"precision\"]:.4f}, Recall: {transductive_std[\"recall\"]:.4f}, F1: {transductive_std[\"f1\"]:.4f}, MRR: {transductive_std[\"mrr\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56140f20-c583-4afd-8650-410e659ca845",
   "metadata": {},
   "source": [
    "Printing the output to a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02e1e8-eda8-473b-af6a-e939d96718ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.../results.txt', 'w') as f:\n",
    "    for line in log_output:\n",
    "        f.write(line + '\\n')\n",
    "    for line in output:\n",
    "        f.write(line + '\\n')\n",
    "for line in output:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
