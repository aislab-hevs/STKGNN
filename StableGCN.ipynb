{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be115c71-ec93-4361-a0fd-12fef37065ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImportLocalData import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b557d873-9352-4bc4-ac75-44b6da768750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BalanceClassDistribution import AdjustClassSamples, NumberOfSamplesClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abfbf7-f53b-4c66-a679-b0a5fc342990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import one of the custom KG files\n",
    "# Call BalanceClass... to handle outliers if you need\n",
    "# Please change path names based on your local files \n",
    "data = loadData('.../node_features.txt', '.../edges.txt', '.../edge_features.txt', '.../node_labels.txt')\n",
    "#data = AdjustClassSamples(data) #this is optional, yet in paper we used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03137ba-fa0f-4bc5-aba7-ace95fd50f62",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685d0ef7-d399-4034-9745-886ca5ed3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import LayerNorm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.utils import subgraph\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdfae1-825b-4e9f-abae-b298996a28b5",
   "metadata": {},
   "source": [
    "Definition of StableGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658dddaa-3bdb-47c0-8247-8ad7ab0cca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #cuda\" if torch.cuda.is_available() else \"cpu\") # if you have gpu you can enable it\n",
    "\n",
    "# GCN with Layer Normalization and Skip Conections brings stabilization and model's name come from this info\n",
    "class StableGCN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super(StableGCN, self).__init__()\n",
    "        self.c1 = GCNConv(in_dim, h_dim)\n",
    "        self.c2 = GCNConv(h_dim, h_dim)\n",
    "        self.n1 = LayerNorm(h_dim)\n",
    "        self.n2 = LayerNorm(h_dim)\n",
    "        self.lin = torch.nn.Linear(h_dim, out_dim)\n",
    "        self.dp = torch.nn.Dropout(0.5)\n",
    "        \n",
    "# Linear layer to align dimensions for the skip connection\n",
    "        self.sk = torch.nn.Linear(in_dim, h_dim)\n",
    "    def forward(self, d):\n",
    "        x, e = d.x.to(device), d.edge_index.to(device)\n",
    "        y = self.sk(x)\n",
    "        x = self.c1(x, e)\n",
    "        x = self.n1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + y # Skip connection\n",
    "        x = self.dp(x)\n",
    "        y = x\n",
    "        x = self.c2(x, e)\n",
    "        x = self.n2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + y # Skip connection\n",
    "        x = self.dp(x)\n",
    "        return self.lin(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7d12c-d1a6-4815-8531-e307aac2649c",
   "metadata": {},
   "source": [
    "Normalization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b3447-bf52-48df-a870-2f3a2c34fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(data.x.cpu().numpy())  # Scaling data on the CPU\n",
    "data.x = torch.tensor(x_scaled, dtype=torch.float).to(device)  # Move data back to GPU\n",
    "\n",
    "# Split data with balanced number of classes before 5-Fold Cross Validation \n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# For keeping inductive and transductive results\n",
    "inductive_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "transductive_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mrr': []}\n",
    "\n",
    "# List to store all fold information and each epoch results for writing to file\n",
    "all_logs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3640826-df22-4e76-a2da-09da7d40b89d",
   "metadata": {},
   "source": [
    "MRR Metric Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405054f0-5209-4b02-8426-b043b21195bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(y_true, y_prob):\n",
    "    rr = []\n",
    "    for true, prob in zip(y_true, y_prob):\n",
    "        order = np.argsort(prob)[::-1]\n",
    "        rank = np.where(order == true)[0][0] + 1\n",
    "        rr.append(1/rank)\n",
    "    return np.mean(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633adc03-38d6-4473-8a92-ce9303b87def",
   "metadata": {},
   "source": [
    "Training Model with 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c902b-65e4-4d11-a8a5-538d7a230e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we apply 5-fold validation and calcuate all performance metrics for each of them to ensure robustness of the results\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(skf.split(torch.arange(data.num_nodes), data.y.cpu().numpy())):\n",
    "    print(f'Fold {fold_idx+1}/5')\n",
    "    all_logs.append(f'Fold {fold_idx+1}/5')\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=data.y[train_idx].cpu().numpy())\n",
    "    train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "    val_idx = torch.tensor(val_idx, dtype=torch.long)\n",
    "    test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "    t_sub = subgraph(train_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    v_sub = subgraph(val_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    ts_sub = subgraph(test_idx, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "    g_sub = subgraph(torch.arange(data.num_nodes), data.edge_index, relabel_nodes=False, num_nodes=data.num_nodes)\n",
    "\n",
    "    train_dat = Data(x=data.x[train_idx], edge_index=t_sub[0], y=data.y[train_idx])\n",
    "    val_dat = Data(x=data.x[val_idx], edge_index=v_sub[0], y=data.y[val_idx])\n",
    "    ind_test_dat = Data(x=data.x[test_idx], edge_index=ts_sub[0], y=data.y[test_idx])\n",
    "    trans_test_dat = Data(x=data.x, edge_index=g_sub[0], y=data.y)\n",
    "    bsz = 32\n",
    "    loader_tr = DataLoader([train_dat], batch_size=bsz, shuffle=True)\n",
    "    loader_val = DataLoader([val_dat], batch_size=bsz)\n",
    "    loader_ti = DataLoader([ind_test_dat], batch_size=bsz)\n",
    "    loader_tt = DataLoader([trans_test_dat], batch_size=bsz)\n",
    "\n",
    "    feat_dim = data.num_node_features\n",
    "    out_c = len(data.y.unique())\n",
    "    hid = 256\n",
    "    net = StableGCN(feat_dim, hid, out_c).to(device)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    lrsc = torch.optim.lr_scheduler.StepLR(opt, step_size=1000, gamma=0.75)\n",
    "    lossf = torch.nn.CrossEntropyLoss()\n",
    "    best_acc = 0\n",
    "    best_weights = None\n",
    "\n",
    "    for ep in range(4000):\n",
    "        net.train()\n",
    "        lval = 0\n",
    "        for b in loader_tr:\n",
    "            b = b.to(device)\n",
    "            o = net(b)\n",
    "            loss = lossf(o, b.y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            lval += loss.item()\n",
    "        lrsc.step()\n",
    "        if (ep+1) % 100 == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                v_loss, v_corr = 0, 0\n",
    "                for vb in loader_val:\n",
    "                    vb = vb.to(device)\n",
    "                    outv = net(vb)\n",
    "                    v_loss += lossf(outv, vb.y).item()\n",
    "                    _, predv = outv.max(dim=1)\n",
    "                    v_corr += predv.eq(vb.y).sum().item()\n",
    "                v_loss /= len(loader_val)\n",
    "                v_acc = v_corr / len(val_dat.y)\n",
    "            log_line = f'Epoch: {ep+1}, Loss: {lval:.4f}, LR: {lrsc.get_last_lr()[0]:.6f}, Val Loss: {v_loss:.4f}, Val Acc: {v_acc:.4f}'\n",
    "            print(log_line)\n",
    "            all_logs.append(log_line)\n",
    "            if v_acc > best_acc:\n",
    "                best_acc = v_acc\n",
    "                best_weights = net.state_dict()\n",
    "            net.train()\n",
    "            \n",
    "    # Inductive Reasoning: All information of test data will not be used during training\n",
    "    # Transductive Reasoning: All node and edge information is accessible during training, except for the labels of the test nodes\n",
    "    for tag, loader, resdict in [\n",
    "        (\"Inductive\", loader_ti, ind_res), (\"Transductive\", loader_tt, trans_res)]:\n",
    "        net.load_state_dict(best_weights)\n",
    "        net.eval()\n",
    "        total, correct, pred_list, label_list, prob_list = 0, 0, [], [], []\n",
    "        with torch.no_grad():\n",
    "            for bt in loader:\n",
    "                bt = bt.to(device)\n",
    "                outt = net(bt)\n",
    "                _, pred = outt.max(dim=1)\n",
    "                total += bt.y.size(0)\n",
    "                correct += pred.eq(bt.y).sum().item()\n",
    "                pred_list.extend(pred.cpu().numpy())\n",
    "                label_list.extend(bt.y.cpu().numpy())\n",
    "                prob_list.extend(F.softmax(outt, dim=1).cpu().numpy())\n",
    "        acc = correct / total if total else 0\n",
    "        prec = precision_score(label_list, pred_list, average='weighted')\n",
    "        rec = recall_score(label_list, pred_list, average='weighted')\n",
    "        f1 = f1_score(label_list, pred_list, average='weighted')\n",
    "        mrr_val = mrr(label_list, prob_list)\n",
    "        resdict['accuracy'].append(acc)\n",
    "        resdict['precision'].append(prec)\n",
    "        resdict['recall'].append(rec)\n",
    "        resdict['f1'].append(f1)\n",
    "        resdict['mrr'].append(mrr_val)\n",
    "        output = (f\"{tag} Fold {fold_idx+1} | Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, MRR: {mrr_val:.4f}\")\n",
    "        print(output)\n",
    "        all_logs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e493f6d-ad68-412a-9a0b-d9904af63224",
   "metadata": {},
   "source": [
    "Calculatıon of means and stadard deviations of performance resuls of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede771f-55d7-462d-9d3c-953655df66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgStd(resdict):\n",
    "    m, s = {}, {}\n",
    "    for k, v in resdict.items():\n",
    "        m[k] = np.mean(v)\n",
    "        s[k] = np.std(v)\n",
    "    return m, s\n",
    "\n",
    "meanInd, stdInd = getAvgStd(ind_res)\n",
    "meanTrans, stdTrans = getAvgStd(trans_res)\n",
    "\n",
    "print('Final Inductive:', meanInd)\n",
    "print('Std Inductive:', stdInd)\n",
    "print('Final Transductive:', meanTrans)\n",
    "print('Std Transductive:', stdTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c6cc8-f02a-46a9-a295-0639c3e9b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnting the output to a .txt file\n",
    "with open('.../results.txt', 'w') as f:\n",
    "    for line in log_output:\n",
    "        f.write(line + '\\n')\n",
    "    for line in output:\n",
    "        f.write(line + '\\n')       \n",
    "for line in output:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
