{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91838f1-1fe7-4e4a-8ba5-a998c3410d69",
   "metadata": {},
   "source": [
    "Import reuired packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbde027-92fd-4dfd-a687-8c558221398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c525d18-e9a1-4f41-83a7-af682526badd",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d092a6f-fd64-4784-b168-6baee7989d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#give proper paths \n",
    "\n",
    "def load_data():\n",
    "    node_features = []\n",
    "    with open('.../node_features.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            node_features.append([float(x) for x in line.strip().split()[1:]])\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    edge_index = []\n",
    "    with open('.../edges.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            edge_index.append([int(x) for x in line.strip().split()])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    edge_attr = []\n",
    "    with open('.../edge_features.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            edge_attr.append([float(x) for x in line.strip().split()[2:]])\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    y = []\n",
    "    with open('.../node_labels.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            y.append(int(line.strip().split()[1]))\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Create batch knowledge\n",
    "    batch = torch.arange(x.size(0), dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, batch=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478de531-7f35-451f-b869-d1d02873a840",
   "metadata": {},
   "source": [
    "We built this module to simulate the temporal continuity between fames as the proposed action recognition module in AKU graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832111cd-9c4b-4a38-86b4-5d68182a7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_edges(data, num_frames=8):\n",
    "    temporal_edges = []\n",
    "    for i in range(data.num_nodes - 1):\n",
    "        temporal_edges.append([i, i+1]) #combine nodes in consecutive frames\n",
    "    temporal_edges = torch.tensor(temporal_edges, dtype=torch.long).t()\n",
    "    data.edge_index = torch.cat([data.edge_index, temporal_edges], dim=1) # combine orijinal edge_index \n",
    "    temporal_attr = torch.ones(temporal_edges.size(1), dtype=torch.float).unsqueeze(-1) ## Attribute for temporal edges (time difference)\n",
    "    # Make edge attribute size compatible\n",
    "    if data.edge_attr is not None and data.edge_attr.dim() > 1:\n",
    "        original_attr = data.edge_attr\n",
    "    else:\n",
    "        original_attr = torch.ones(data.edge_index.size(1) - temporal_edges.size(1), 1, dtype=torch.float)\n",
    "    \n",
    "    data.edge_attr = torch.cat([original_attr, temporal_attr], dim=0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080af3d6-0a9d-460e-a04b-3693eec5f6ee",
   "metadata": {},
   "source": [
    "AKU-INSPRED MODEL OVER OUR STKG DATA (Since AKU is multimodal, we only concentated on action recognition module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63470fbe-e194-473c-b705-23a1894e2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCNLayer(GCNConv):\n",
    "    def forward(self, features, indices, _=None):\n",
    "        return super().forward(features, indices)\n",
    "\n",
    "class AKUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn1 = SimpleGCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = SimpleGCNLayer(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2),nn.ReLU(),nn.Linear(hidden_dim // 2, num_classes))\n",
    "\n",
    "        self.uncertainty_head = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2),nn.ReLU(),nn.Linear(hidden_dim // 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, data):\n",
    "        device = data.x.device\n",
    "        temporal_edges = torch.stack([torch.arange(data.num_nodes - 1, device=device),torch.arange(1, data.num_nodes, device=device)])\n",
    "        edge_index = torch.cat([data.edge_index, temporal_edges], dim=1)\n",
    "\n",
    "        # GCNConv don't use edge_attr\n",
    "        x = F.relu(self.gcn1(data.x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "\n",
    "        x = global_mean_pool(x, data.batch if hasattr(data, 'batch') else torch.zeros(data.num_nodes, dtype=torch.long, device=device))\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        uncertainty = self.uncertainty_head(x).squeeze(-1)\n",
    "        return logits, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ecae5-dda1-409d-a263-8330650b2d5e",
   "metadata": {},
   "source": [
    "Loss function, we mimic the uncertainty learning in AKU paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271373c3-2435-454e-8e2b-5f63d0072713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_loss(pred_logits, uncertainty, targets, alpha=0.5):\n",
    "    ce_loss = F.cross_entropy(pred_logits, targets)\n",
    "        pred_probs = F.softmax(pred_logits, dim=1)\n",
    "    max_probs = pred_probs.max(dim=1)[0]\n",
    "    unc_loss = F.mse_loss(uncertainty, 1 - max_probs.detach())\n",
    "    \n",
    "    return ce_loss + alpha * unc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb41600-5866-4136-8ed3-19a785605777",
   "metadata": {},
   "source": [
    "TRAIN / TEST / EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9c74f-d9a5-496b-8806-299226ce0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, data, optimizer, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits, uncertainty = model(data)\n",
    "    loss = uncertainty_loss(logits[train_idx], uncertainty[train_idx], data.y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, idx, uncertainty_threshold=0.4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, uncertainty = model(data)\n",
    "        pred = logits.argmax(dim=1) \n",
    "        # Top-1 accuracy calculation\n",
    "        top1_acc = (pred[idx] == data.y[idx]).float().mean().item()\n",
    "        # Top-5 accuracy calculation\n",
    "        top5_acc = top_k_accuracy_score(data.y[idx].cpu().numpy(), logits[idx].cpu().numpy(), k=5, labels=np.arange(logits.size(1)))\n",
    "        \n",
    "        # Filtering based on uncertainty\n",
    "        mask = uncertainty < uncertainty_threshold\n",
    "        filtered_idx = idx[mask[idx]]\n",
    "        if len(filtered_idx) > 0:\n",
    "            acc = (pred[filtered_idx] == data.y[filtered_idx]).float().mean().item()\n",
    "            top5_confident = top_k_accuracy_score(data.y[filtered_idx].cpu().numpy(), logits[filtered_idx].cpu().numpy(), k=5, labels=np.arange(logits.size(1)))\n",
    "        else:\n",
    "            acc = 0.0\n",
    "            top5_confident = 0.0\n",
    "            \n",
    "        return (top1_acc, top5_acc)  # Top1 and Top5 Acc (all samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779bfc3-004f-4e0a-a40b-c515e6e13518",
   "metadata": {},
   "source": [
    "The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c705a4-81af-4331-823f-482aa37b9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    data = load_data()\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Split data with proper distribution betweeen classes\n",
    "    indices = list(range(data.num_nodes))\n",
    "    train_idx, test_val_idx = train_test_split(indices, test_size=0.2, stratify=data.y.cpu().numpy())\n",
    "    val_idx, test_idx = train_test_split(test_val_idx, test_size=0.5, stratify=data.y[test_val_idx].cpu().numpy())\n",
    "    train_idx = torch.tensor(train_idx, dtype=torch.long, device=device)\n",
    "    val_idx = torch.tensor(val_idx, dtype=torch.long, device=device)\n",
    "    test_idx = torch.tensor(test_idx, dtype=torch.long, device=device)\n",
    "\n",
    "    num_classes = int(data.y.max().item()) + 1\n",
    "    model = AKUModel(input_dim=data.x.size(1), hidden_dim=64, num_classes=num_classes).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=1, epochs=4000, pct_start=0.3, anneal_strategy='cos')\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_top5 = 0\n",
    "    best_model = None\n",
    "    patience = 3600\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, 4001):\n",
    "        loss = train(model, data, optimizer, train_idx)\n",
    "        val_top1, val_top5, val_acc, val_top5_confident, val_confident_ratio = evaluate(model, data, val_idx)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_top5 = val_top5\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch} as validation accuracy didn't improve for {patience} evaluations\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "                  f\"Val Top1: {val_top1*100:.2f}% | Val Top5: {val_top5*100:.2f}% | \"\n",
    "                  f\"Val Acc: {val_acc*100:.2f}% | Val Top5 Conf: {val_top5_confident*100:.2f}% | \"\n",
    "                  f\"Confident: {val_confident_ratio*100:.1f}% | LR: {current_lr:.6f}\")\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    test_top1, test_top5, test_acc, test_top5_confident, test_confident_ratio = evaluate(model, data, test_idx)\n",
    "    print(f\"\\n✅ Final Test Results:\")\n",
    "    print(f\"Top-1 Accuracy: {test_top1*100:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {test_top5*100:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca017c-3235-4f4f-b36f-0553ec67c66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c471145-057e-4afc-a062-e6b3d8e05894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31509d76-09ef-403c-b847-350d18027e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
